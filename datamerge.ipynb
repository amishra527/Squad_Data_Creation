{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def extract_features(input_folder, output_file, encoding='utf-8'):\n",
    "    data = []\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.endswith('.json'):\n",
    "            with open(os.path.join(input_folder, filename), 'r', encoding=encoding) as f:\n",
    "                file_data = json.load(f)\n",
    "                for item in file_data['data']:\n",
    "                    title = item['title']\n",
    "                    for paragraph in item['paragraphs']:\n",
    "                        context = paragraph['context']\n",
    "                        for qa in paragraph['qas']:\n",
    "                            id = qa['id']\n",
    "                            question = qa['question']\n",
    "                            answers = []\n",
    "                            for answer in qa['answers']:\n",
    "                                answers.append({\n",
    "                                    'text': answer['text'],\n",
    "                                    'answer_start': answer['answer_start']\n",
    "                                })\n",
    "                            data.append({\n",
    "                                'id': id,\n",
    "                                'title': title,\n",
    "                                'context': context,\n",
    "                                'question': question,\n",
    "                                'answers': answers\n",
    "                            })\n",
    "    with open(output_file, 'w', encoding=encoding) as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "extract_features('MhaInformatics2trainingdatasetMinistryWise', 'MhaInformatics_merge.json', encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def remove_duplicate_questions(input_file, output_file, encoding='utf-8'):\n",
    "    with open(input_file, 'r', encoding=encoding) as f:\n",
    "        data = json.load(f)\n",
    "    unique_questions = set()\n",
    "    new_data = []\n",
    "    for item in data:\n",
    "        question = item['question']\n",
    "        if question not in unique_questions:\n",
    "            unique_questions.add(question)\n",
    "            new_data.append(item)\n",
    "    with open(output_file, 'w', encoding=encoding) as f:\n",
    "        json.dump(new_data, f, indent=4)\n",
    "\n",
    "remove_duplicate_questions('MhaInformatics_merge.json', 'MhaInformatics_merge_unique.json', encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# context = paragraph.get('context')\n",
    "# if context:\n",
    "#     # do something with context\n",
    "# else:\n",
    "#     # handle the case where context is not found\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if 'context' in paragraph:\n",
    "#     context = paragraph['context']\n",
    "#     # do something with context\n",
    "# else:\n",
    "#     # handle the case where context is not found\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def extract_features(input_folder, output_file, encoding='utf-8'):\n",
    "    data = []\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.endswith('.json'):\n",
    "            with open(os.path.join(input_folder, filename), 'r', encoding=encoding) as f:\n",
    "                file_data = json.load(f)\n",
    "                for item in file_data['data']:\n",
    "                    title = item['title']\n",
    "                    for paragraph in item['paragraphs']:\n",
    "                        context = paragraph.get('context')\n",
    "                        if context:\n",
    "                            for qa in paragraph['qas']:\n",
    "                                id = qa['id']\n",
    "                                question = qa['question']\n",
    "                                answers = []\n",
    "                                for answer in qa['answers']:\n",
    "                                    answers.append({\n",
    "                                        'text': answer['text'],\n",
    "                                        'answer_start': answer['answer_start']\n",
    "                                    })\n",
    "                                data.append({\n",
    "                                    'id': id,\n",
    "                                    'title': title,\n",
    "                                    'context': context,\n",
    "                                    'question': question,\n",
    "                                    'answers': answers\n",
    "                                })\n",
    "    with open(output_file, 'w', encoding=encoding) as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "extract_features('Xa', 'Xa_merge.json', encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def remove_duplicate_questions(input_file, output_file, encoding='utf-8'):\n",
    "    with open(input_file, 'r', encoding=encoding) as f:\n",
    "        data = json.load(f)\n",
    "    unique_questions = set()\n",
    "    new_data = []\n",
    "    for item in data:\n",
    "        question = item['question']\n",
    "        if question not in unique_questions:\n",
    "            unique_questions.add(question)\n",
    "            new_data.append(item)\n",
    "    with open(output_file, 'w', encoding=encoding) as f:\n",
    "        json.dump(new_data, f, indent=4)\n",
    "\n",
    "remove_duplicate_questions('Xa_merge.json', 'Xa_merge_unique.json', encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def merge_json_files(input_folder, output_file):\n",
    "    merged_data = []\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.endswith('.json'):\n",
    "            with open(os.path.join(input_folder, filename), 'r') as f:\n",
    "                file_data = json.load(f)\n",
    "                merged_data.extend(file_data)\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(merged_data, f, indent=4)\n",
    "\n",
    "# Example usage\n",
    "merge_json_files('XFinal', 'XFinal_merge.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def remove_empty_answers(input_file, output_file):\n",
    "    with open(input_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    filtered_data = [item for item in data if item['answers']]\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(filtered_data, f, indent=4)\n",
    "\n",
    "# Example usage\n",
    "remove_empty_answers('XFinal_merge.json', 'XFinal_clean.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
